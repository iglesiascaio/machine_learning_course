{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# The first two columns contains the exam scores and the third column\n",
    "# contains the label.\n",
    "data = np.loadtxt('data1.txt', delimiter=',')\n",
    " \n",
    "X = data[:, 0:2]\n",
    "y = data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data \n",
    "pos = np.where(y == 1)\n",
    "neg = np.where(y == 0)\n",
    "plt.scatter(X[pos, 0], X[pos, 1], marker='o', c='b')\n",
    "plt.scatter(X[neg, 0], X[neg, 1], marker='x', c='r')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.legend(['Admitted', 'Not Admitted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add intercept term to X\n",
    "X_new = np.ones((X.shape[0], 3))\n",
    "X_new[:, 1:] = X\n",
    "X = X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression may be explained from a probabilistic perspective. \n",
    "\n",
    "Let us denote $x_{1:m}, y_{1:m} \\sim X, Y$ i.i.d observations of $X \\in \\mathbb{R}^p, Y \\in \\{0,1\\}$.\n",
    "\n",
    "### The model\n",
    "\n",
    "Let us consider the following regression model to explain the data\n",
    "\\begin{equation*}\n",
    "    P_{Y|X=x} = \\mathrm{Ber}(p = \\sigma(x^\\top \\theta))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing the likelihood\n",
    "Under this model, the probability of observing $(x_i, y_i)$ is\n",
    "\\begin{align*}\n",
    "    p_{Y|X=x_i}(y_i) &= p^{y_i} (1-p)^{1-y_i} \\\\\n",
    "    &= \\sigma(x_i^\\top\\theta)^{y_i} (1-\\sigma(x_i^\\top\\theta))^{1-y_i}\n",
    "\\end{align*}\n",
    "\n",
    "The optimal value of $\\theta$ is chosen to maximize the likelihood of observations that is\n",
    "\\begin{align*}\n",
    "    \\theta^* &= \\underset{\\theta}{\\text{arg max}} \\prod_{i=1}^m p_{Y|X=x_i}(y_i) \\\\\n",
    "    &= \\underset{\\theta}{\\text{arg max}} \\sum_{i=1}^m \\log p_{Y|X=x_i}(y_i) \\\\\n",
    "    &= \\underset{\\theta}{\\text{arg max}} \\sum_{i=1}^m y_i \\log \\sigma(\\theta^\\top x_i) + (1-y_i) \\log (1-\\sigma(\\theta^\\top x_i)) \\\\\n",
    "    &= \\underset{\\theta}{\\text{arg min}} -\\frac{1}{m} \\sum_{i=1}^m y_i \\log \\sigma(\\theta^\\top x_i) + (1-y_i) \\log (1-\\sigma(\\theta^\\top x_i)) \\\\\n",
    "    &= \\underset{\\theta}{\\text{arg min}} \\quad J(\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Logit function\n",
    "def logit(z):\n",
    "    return np.log(z/(1-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sigmoid and logit\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\n",
    "Xplot = np.linspace(-5, 5, 100)\n",
    "Yplot = sigmoid(Xplot)\n",
    "ax[0].plot(Xplot, Yplot, color='dodgerblue', label='sigmoid')\n",
    "Xplot = np.linspace(0.001, 0.999, 100)\n",
    "Yplot = logit(Xplot)\n",
    "ax[1].plot(Xplot, Yplot, color='indianred', label='logit')\n",
    "for i in range(2):\n",
    "    ax[i].legend(loc='best', fontsize=15)\n",
    "    ax[i].spines['top'].set_visible(False)\n",
    "    ax[i].spines['right'].set_visible(False)\n",
    "    ax[i].spines['left'].set_position('center')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sigmoid():\n",
    "    # scalar test\n",
    "    z, s = 0.5, 0.622459\n",
    "    if np.abs(sigmoid(z)-s) < 1e-5:\n",
    "        print(\"scalar test passed\")\n",
    "    else:\n",
    "        raise ValueError(\"scalar test not passed!\")\n",
    "        \n",
    "    # vector test\n",
    "    z, s = np.array([1,-1,0.5]), np.array([0.73105858, 0.26894142, 0.62245933])\n",
    "    if np.sum(np.abs(sigmoid(z)-s)) < 1e-5:\n",
    "        print(\"vector test passed\")\n",
    "    else:\n",
    "        raise ValueError(\"vector test not passed!\")\n",
    "        \n",
    "test_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The cost function $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is the following\n",
    "\n",
    "\\begin{equation}\n",
    "    J_\\theta = -\\frac{1}{m} \\sum_{i=1}^m y_i \\log(\\sigma(x_i^T\\theta)) + (1-y_i) \\log(1 -\\sigma(x_i^T\\theta))\n",
    "\\end{equation}\n",
    "\n",
    "In a matrix notation\n",
    "\\begin{equation}\n",
    "    \\text{PLEASE FIND THE MATRIX FORM}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence on X,y is implicit\n",
    "# X, y are defined globally\n",
    "def computeCost(theta):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_computeCost():\n",
    "    # test 1\n",
    "    theta, cost = [0, 0, 0], 0.6931471\n",
    "    if np.abs(computeCost(theta)-cost) < 1e-5:\n",
    "        print(\"test 1 passed\")\n",
    "    else:\n",
    "        raise ValueError(\"test 1 not passed!\")\n",
    "        \n",
    "    # test 2\n",
    "    theta, cost = [-0.01, 0.05, 0], 1.092916\n",
    "    if np.abs(computeCost(theta)-cost) < 1e-5:\n",
    "        print(\"test 2 passed\")\n",
    "    else:\n",
    "        raise ValueError(\"test 2 not passed!\")\n",
    "        \n",
    "test_computeCost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The gradient function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost function is given by\n",
    "\\begin{equation}\n",
    "    \\text{PLEASE FIND THE GRADIENT}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence on X,y is implicit\n",
    "# X, y are defined globally\n",
    "def computeGrad(theta):\n",
    "    # Computes the gradient of the cost with respect to\n",
    "    # the parameters.\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_computeGrad():\n",
    "    # test 1\n",
    "    theta, grad = [0, 0, 0], np.array([ -0.1, -12.00921659, -11.26284221])\n",
    "    if np.sum(np.abs(computeGrad(theta)-grad)) < 1e-5:\n",
    "        print(\"test 1 passed\")\n",
    "    else:\n",
    "        raise ValueError(\"test 1 not passed!\")\n",
    "        \n",
    "    # test 2\n",
    "    theta, grad = [0.02, 0, -0.04], np.array([-0.51775522, -39.39901278, -39.85199474])\n",
    "    if np.sum(np.abs(computeGrad(theta)-grad)) < 1e-5:\n",
    "        print(\"test 2 passed\")\n",
    "    else:\n",
    "        raise ValueError(\"test 2 not passed!\")\n",
    "        \n",
    "test_computeGrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Solve the optimization problem (i.e fit the model to the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run minimize() to obtain the optimal theta\n",
    "Result = op.minimize(fun=computeCost, x0=theta, \n",
    "                     method = 'TNC', jac=computeGrad);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualize the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(\\theta^T x) = 0.5 \\iff \\theta^T x = 0\n",
    "\\end{equation}\n",
    "\n",
    "i.e\n",
    "\\begin{equation}\n",
    "  x_2 = -\\frac{\\theta_1}{\\theta_2}x_1 - \\frac{\\theta_0}{\\theta_2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "plot_x = np.array([min(X[:, 1]), max(X[:, 1])])\n",
    "plot_y = (- 1.0 / theta[2]) * (theta[1] * plot_x + theta[0])\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.scatter(X[pos, 1], X[pos, 2], marker='o', c='b')\n",
    "plt.scatter(X[neg, 1], X[neg, 2], marker='x', c='r')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.legend(['Decision Boundary', 'Admitted', 'Not Admitted'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    # Predict whether the label is 0 or 1 using learned logistic \n",
    "    # regression parameters theta. The threshold is set at 0.5\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict():\n",
    "    # test 1\n",
    "    X_test, y_pred = np.array([[1,40, 20], [1,40,80], [1,80,80]]), np.array([0,0,1])\n",
    "    if np.sum(np.abs(y_pred - predict(Result.x, X_test))) == 0:\n",
    "        print(\"test 1 passed\")\n",
    "    else:\n",
    "        raise ValueError(\"test 1 not passed!\")\n",
    "\n",
    "    # test 2\n",
    "    X_test, y_pred = np.array([[1,70, 50], [1,70,40]]), np.array([0,0])\n",
    "    if np.sum(np.abs(y_pred - predict(Result.x, X_test))) == 0:\n",
    "        print(\"test 2 passed\")\n",
    "    else:\n",
    "        raise ValueError(\"test 2 not passed!\")\n",
    "        \n",
    "test_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy on the training set\n",
    "p = predict(Result.x, X)\n",
    "counter = 0\n",
    "for i in range(y.size):\n",
    "    if p[i] == y[i]:\n",
    "        counter += 1\n",
    "print('Train Accuracy: {:.2f}'.format(counter / float(y.size) * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
