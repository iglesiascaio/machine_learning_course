{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# https://www.python.org/dev/peps/pep-0008#introduction<BR>\n",
    "# http://scikit-learn.org/<BR>\n",
    "# http://pandas.pydata.org/<BR>\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fetch the data and load it in pandas\n",
    "data = pd.read_csv('train.csv')\n",
    "print(\"Size of the data: \", data.shape)\n",
    "\n",
    "#%%\n",
    "# See data (five rows) using pandas tools\n",
    "#print data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Prepare input to scikit and train and test cut\n",
    "\n",
    "binary_data = data[np.logical_or(data['Cover_Type'] == 1, data['Cover_Type'] == 2)] # two-class classification set\n",
    "X = binary_data.drop('Cover_Type', axis=1).values\n",
    "y = binary_data['Cover_Type'].values\n",
    "print(np.unique(y))\n",
    "y = 2 * y - 3 # converting labels from [1,2] to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import cross validation tools from scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "### Train a single decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "# Train the classifier and print training time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Do classification on the test dataset and print classification results\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = data['Cover_Type'].unique().astype(str).sort()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# Compute accuracy of the classifier (correctly classified instances)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===================================================================\n",
    "#%%\n",
    "### Train AdaBoost\n",
    "\n",
    "# Your first exercise is to program AdaBoost.\n",
    "# You can call *DecisionTreeClassifier* as above, \n",
    "# but you have to figure out how to pass the weight vector (for weighted classification) \n",
    "# to the *fit* function using the help pages of scikit-learn. At the end of \n",
    "# the loop, compute the training and test errors so the last section of the code can \n",
    "# plot the lerning curves. \n",
    "# \n",
    "# Once the code is finished, play around with the hyperparameters (D and T), \n",
    "# and try to understand what is happening.\n",
    "\n",
    "D = 2 # tree depth\n",
    "T = 10 # number of trees\n",
    "w = np.ones(X_train.shape[0]) / X_train.shape[0] # weight initialization\n",
    "training_scores = np.zeros(X_train.shape[0]) # init scores with 0\n",
    "test_scores     = np.zeros(X_test.shape[0])\n",
    " \n",
    "# init errors\n",
    "training_errors = []\n",
    "test_errors = []\n",
    "\n",
    "#===============================\n",
    "for t in range(T):\n",
    "    \n",
    "    # Your code should go here\n",
    "    \n",
    "\n",
    "#===============================\n",
    "\n",
    "#  Plot training and test error    \n",
    "plt.plot(training_errors, label=\"training error\")\n",
    "plt.plot(test_errors, label=\"test error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#===================================================================\n",
    "#%%\n",
    "### Optional part\n",
    "### Optimize AdaBoost\n",
    "\n",
    "# Your final exercise is to optimize the tree depth in AdaBoost. \n",
    "# Copy-paste your AdaBoost code into a function, and call it with different tree depths \n",
    "# and, for simplicity, with T = 100 iterations (number of trees). Plot the final \n",
    "# test error vs the tree depth. Discuss the plot.\n",
    "\n",
    "#===============================\n",
    "\n",
    "# Your code should go here\n",
    "    \n",
    "\n",
    "#==============================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
